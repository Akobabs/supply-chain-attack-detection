{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkdq1NpEKLFhV8VHEf/xLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akobabs/supply-chain-attack-detection/blob/main/Rest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDZqKRqe8OHZ",
        "outputId": "446acd72-c8b1-4620-8911-f307b6fe4418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clprgaTh8w67",
        "outputId": "f2bcfe75-1619-423f-92bf-48fb57e03af2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zipped dataset in Google Drive\n",
        "zip_path = '/content/drive/My Drive/UNSW_DATASET/OneDrive_2025-05-07.zip'\n",
        "extract_path = '/content/UNSW_DATASET'\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Verify the extracted files\n",
        "print(\"Extracted files:\", os.listdir(extract_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msbgIljT_fFr",
        "outputId": "526f73cd-9b65-4bc0-ab0b-3b164d350f8d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['CSV Files']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "csv_files_path = '/content/UNSW_DATASET/CSV Files'\n",
        "print(os.listdir(csv_files_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk1IfqUn_73l",
        "outputId": "71ddba30-4cfe-4f70-abd6-7c552617b469"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['UNSW-NB15_LIST_EVENTS.csv', 'UNSW-NB15_4.csv', 'Training and Testing Sets', 'NUSW-NB15_features.csv', 'The UNSW-NB15 description.pdf', 'NUSW-NB15_GT.csv', 'UNSW-NB15_2.csv', 'UNSW-NB15_3.csv', 'UNSW-NB15_1.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main preprocessing pipeline.\"\"\"\n",
        "    try:\n",
        "        # Define file paths with the correct directory\n",
        "        file_paths = [\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_1.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_2.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_3.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_4.csv'\n",
        "        ]\n",
        "\n",
        "        # Run preprocessing steps\n",
        "        df = load_and_combine_data(file_paths)\n",
        "        df = handle_missing_values(df)\n",
        "        df = filter_relevant_attacks(df)\n",
        "        df = select_features(df)\n",
        "        df = encode_categorical(df)\n",
        "        df = balance_classes(df)\n",
        "        df = normalize_features(df)\n",
        "\n",
        "        # Save final preprocessed dataset\n",
        "        df.to_csv('unsw_nb15_preprocessed.csv', index=False)\n",
        "        logging.info(\"Preprocessing complete. Final dataset saved as 'unsw_nb15_preprocessed.csv'\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Preprocessing failed: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "rxTN8mwlAl29"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='preprocessing.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Define column names for UNSW-NB15\n",
        "COLUMNS = [\n",
        "    'srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes', 'sttl',\n",
        "    'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin',\n",
        "    'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit',\n",
        "    'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports',\n",
        "    'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src',\n",
        "    'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
        "    'ct_dst_src_ltm', 'attack_cat', 'Label'\n",
        "]\n",
        "\n",
        "def load_and_combine_data(file_paths):\n",
        "    \"\"\"Load and combine UNSW-NB15 CSV files.\"\"\"\n",
        "    try:\n",
        "        dfs = [pd.read_csv(file, names=COLUMNS, low_memory=False) for file in file_paths]\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "        logging.info(f\"Combined {len(dfs)} files. Total records: {df.shape[0]}\")\n",
        "        df.to_csv('combined_raw.csv', index=False)  # Save intermediate file\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error combining files: {e}\")\n",
        "        raise\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    \"\"\"Handle missing values in the dataset.\"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Missing values before: {df.isnull().sum().sum()}\")\n",
        "        df['attack_cat'] = df['attack_cat'].fillna('Normal')\n",
        "        df = df.dropna(subset=['Label'])\n",
        "        df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "        logging.info(f\"Missing values after: {df.isnull().sum().sum()}\")\n",
        "        df.to_csv('missing_handled.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error handling missing values: {e}\")\n",
        "        raise\n",
        "\n",
        "def filter_relevant_attacks(df):\n",
        "    \"\"\"Filter for Backdoors and Reconnaissance attacks.\"\"\"\n",
        "    try:\n",
        "        relevant_attacks = ['Normal', 'Backdoors', 'Reconnaissance']\n",
        "        df = df[df['attack_cat'].isin(relevant_attacks)]\n",
        "        df['binary_label'] = df['attack_cat'].apply(lambda x: 0 if x == 'Normal' else 1)\n",
        "        logging.info(f\"Filtered attacks. Records: {df.shape[0]}, Classes: {df['binary_label'].value_counts().to_dict()}\")\n",
        "        df.to_csv('filtered_attacks.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error filtering attacks: {e}\")\n",
        "        raise\n",
        "\n",
        "def select_features(df):\n",
        "    \"\"\"Select time and content features for supply-chain attack detection.\"\"\"\n",
        "    try:\n",
        "        features = [\n",
        "            'proto', 'service', 'dur', 'sbytes', 'dbytes', 'Sload', 'Dload',\n",
        "            'smeansz', 'dmeansz', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'binary_label'\n",
        "        ]\n",
        "        df = df[features]\n",
        "        logging.info(f\"Selected features: {features}\")\n",
        "        df.to_csv('features_selected.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error selecting features: {e}\")\n",
        "        raise\n",
        "\n",
        "def encode_categorical(df):\n",
        "    \"\"\"Encode categorical features.\"\"\"\n",
        "    try:\n",
        "        categorical_cols = ['proto', 'service']\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "            logging.info(f\"Encoded column: {col}\")\n",
        "        df.to_csv('encoded_categorical.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error encoding categorical features: {e}\")\n",
        "        raise\n",
        "\n",
        "def balance_classes(df):\n",
        "    \"\"\"Balance classes using SMOTE.\"\"\"\n",
        "    try:\n",
        "        X = df.drop('binary_label', axis=1)\n",
        "        y = df['binary_label']\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "        df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='binary_label')], axis=1)\n",
        "        logging.info(f\"Balanced classes. New records: {df_resampled.shape[0]}, Classes: {df_resampled['binary_label'].value_counts().to_dict()}\")\n",
        "        df_resampled.to_csv('balanced_classes.csv', index=False)\n",
        "        return df_resampled\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error balancing classes: {e}\")\n",
        "        raise\n",
        "\n",
        "def normalize_features(df):\n",
        "    \"\"\"Normalize numerical features.\"\"\"\n",
        "    try:\n",
        "        X = df.drop('binary_label', axis=1)\n",
        "        scaler = StandardScaler()\n",
        "        X_normalized = scaler.fit_transform(X)\n",
        "        df_normalized = pd.concat([pd.DataFrame(X_normalized, columns=X.columns), df['binary_label'].reset_index(drop=True)], axis=1)\n",
        "        logging.info(\"Normalized features\")\n",
        "        df_normalized.to_csv('normalized_features.csv', index=False)\n",
        "        return df_normalized\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error normalizing features: {e}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main preprocessing pipeline.\"\"\"\n",
        "    try:\n",
        "        # Define file paths with the correct directory\n",
        "        file_paths = [\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_1.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_2.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_3.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_4.csv'\n",
        "        ]\n",
        "\n",
        "        # Run preprocessing steps\n",
        "        df = load_and_combine_data(file_paths)\n",
        "        df = handle_missing_values(df)\n",
        "        df = filter_relevant_attacks(df)\n",
        "        df = select_features(df)\n",
        "        df = encode_categorical(df)\n",
        "        df = balance_classes(df)\n",
        "        df = normalize_features(df)\n",
        "\n",
        "        # Save final preprocessed dataset\n",
        "        df.to_csv('unsw_nb15_preprocessed.csv', index=False)\n",
        "        logging.info(\"Preprocessing complete. Final dataset saved as 'unsw_nb15_preprocessed.csv'\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Preprocessing failed: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMhJjxMtA7W1",
        "outputId": "9783c693-bc2e-45b8-809c-8fc6450bcb46"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-184b88321c5c>:57: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['binary_label'] = df['attack_cat'].apply(lambda x: 0 if x == 'Normal' else 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install xgboost --upgrade\n",
        "!pip install shap\n",
        "!pip install joblib\n",
        "\n",
        "print(\"Training model with GPU acceleration...\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch  # For GPU check\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import joblib\n",
        "\n",
        "# Verify GPU availability\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('metrics', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='training.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def load_preprocessed_data(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        logging.info(f\"Loaded preprocessed data: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_supervised_model(X_train, y_train, X_test, y_test):\n",
        "    try:\n",
        "        rf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor',\n",
        "                           use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred = rf.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        fpr = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
        "\n",
        "        logging.info(f\"Supervised Model - Accuracy: {accuracy:.2f}, F1-Score: {f1:.2f}, FPR: {fpr:.2f}\")\n",
        "\n",
        "        # Save confusion matrix plot\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.savefig('plots/confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Save model\n",
        "        joblib.dump(rf, 'models/xgb_supervised_model.pkl')\n",
        "\n",
        "        return rf, {'accuracy': accuracy, 'f1_score': f1, 'fpr': fpr}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training supervised model: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_unsupervised_model(X):\n",
        "    try:\n",
        "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "        anomalies = iso_forest.fit_predict(X)\n",
        "\n",
        "        df_plot = pd.DataFrame(X, columns=X.columns)\n",
        "        df_plot['Anomaly'] = anomalies\n",
        "        sns.scatterplot(data=df_plot, x='dur', y='sbytes', hue='Anomaly', palette={-1: 'red', 1: 'blue'})\n",
        "        plt.title('Anomaly Detection with Isolation Forest')\n",
        "        plt.savefig('plots/anomaly_plot.png')\n",
        "        plt.close()\n",
        "\n",
        "        joblib.dump(iso_forest, 'models/isolation_forest_model.pkl')\n",
        "\n",
        "        logging.info(\"Unsupervised model trained and anomaly plot saved\")\n",
        "        return iso_forest\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training unsupervised model: {e}\")\n",
        "        raise\n",
        "\n",
        "def explain_model(rf, X_test):\n",
        "    try:\n",
        "        explainer = shap.Explainer(rf)\n",
        "        shap_values = explainer(X_test)\n",
        "        shap.summary_plot(shap_values, X_test, show=False)\n",
        "        plt.savefig('plots/shap_summary.png')\n",
        "        plt.close()\n",
        "        logging.info(\"SHAP summary plot saved\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating SHAP explanations: {e}\")\n",
        "        raise\n",
        "\n",
        "def test_adversarial_robustness(rf, X_test, y_test):\n",
        "    try:\n",
        "        X_adv = X_test + np.random.normal(0, 0.1, X_test.shape)\n",
        "        y_pred_adv = rf.predict(X_adv)\n",
        "        adv_accuracy = accuracy_score(y_test, y_pred_adv)\n",
        "        logging.info(f\"Adversarial Accuracy: {adv_accuracy:.2f}\")\n",
        "        return adv_accuracy\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error testing adversarial robustness: {e}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        df = load_preprocessed_data('/content/unsw_nb15_preprocessed.csv')\n",
        "\n",
        "        # Optional: Downsample if dataset is too large\n",
        "        logging.info(f\"Original dataset size: {df.shape}\")\n",
        "        if df.shape[0] > 100000:  # Adjust threshold as needed\n",
        "            df = df.sample(100000, random_state=42)\n",
        "            logging.info(f\"Downsampled dataset to: {df.shape}\")\n",
        "\n",
        "        X = df.drop('binary_label', axis=1)\n",
        "        y = df['binary_label']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        logging.info(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "        rf, metrics = train_supervised_model(X_train, y_train, X_test, y_test)\n",
        "        iso_forest = train_unsupervised_model(X)\n",
        "        explain_model(rf, X_test)\n",
        "        adv_accuracy = test_adversarial_robustness(rf, X_test, y_test)\n",
        "\n",
        "        with open('metrics/metrics.txt', 'w') as f:\n",
        "            f.write(f\"Accuracy: {metrics['accuracy']:.2f}\\n\")\n",
        "            f.write(f\"F1-Score: {metrics['f1_score']:.2f}\\n\")\n",
        "            f.write(f\"False Positive Rate: {metrics['fpr']:.2f}\\n\")\n",
        "            f.write(f\"Adversarial Accuracy: {adv_accuracy:.2f}\\n\")\n",
        "\n",
        "        logging.info(\"Training complete. All metrics, plots, and models saved.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Training failed: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM9u6XwNN9un",
        "outputId": "a6aa2d59-890f-48f8-e513-1972d1789d43"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.13.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Training model with GPU acceleration...\n",
            "GPU available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:29:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:29:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [09:29:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install xgboost --upgrade\n",
        "!pip install shap\n",
        "!pip install joblib\n",
        "\n",
        "print(\"Training model with GPU acceleration...\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch  # For GPU check\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import joblib\n",
        "\n",
        "# Verify GPU availability\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('metrics', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='training.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def load_preprocessed_data(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        logging.info(f\"Loaded preprocessed data: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_supervised_model(X_train, y_train, X_test, y_test):\n",
        "    try:\n",
        "        rf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor',\n",
        "                           use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred = rf.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        fpr = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
        "\n",
        "        logging.info(f\"Supervised Model - Accuracy: {accuracy:.2f}, F1-Score: {f1:.2f}, FPR: {fpr:.2f}\")\n",
        "\n",
        "        # Save confusion matrix plot\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.savefig('plots/confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Save model\n",
        "        joblib.dump(rf, 'models/xgb_supervised_model.pkl')\n",
        "\n",
        "        return rf, {'accuracy': accuracy, 'f1_score': f1, 'fpr': fpr}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training supervised model: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_unsupervised_model(X):\n",
        "    try:\n",
        "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "        anomalies = iso_forest.fit_predict(X)\n",
        "\n",
        "        df_plot = pd.DataFrame(X, columns=X.columns)\n",
        "        df_plot['Anomaly'] = anomalies\n",
        "        sns.scatterplot(data=df_plot, x='dur', y='sbytes', hue='Anomaly', palette={-1: 'red', 1: 'blue'})\n",
        "        plt.title('Anomaly Detection with Isolation Forest')\n",
        "        plt.savefig('plots/anomaly_plot.png')\n",
        "        plt.close()\n",
        "\n",
        "        joblib.dump(iso_forest, 'models/isolation_forest_model.pkl')\n",
        "\n",
        "        logging.info(\"Unsupervised model trained and anomaly plot saved\")\n",
        "        return iso_forest\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training unsupervised model: {e}\")\n",
        "        raise\n",
        "\n",
        "def explain_model(rf, X_test):\n",
        "    try:\n",
        "        explainer = shap.Explainer(rf)\n",
        "        shap_values = explainer(X_test)\n",
        "        shap.summary_plot(shap_values, X_test, show=False)\n",
        "        plt.savefig('plots/shap_summary.png')\n",
        "        plt.close()\n",
        "        logging.info(\"SHAP summary plot saved\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating SHAP explanations: {e}\")\n",
        "        raise\n",
        "\n",
        "def test_adversarial_robustness(rf, X_test, y_test):\n",
        "    try:\n",
        "        X_adv = X_test + np.random.normal(0, 0.1, X_test.shape)\n",
        "        y_pred_adv = rf.predict(X_adv)\n",
        "        adv_accuracy = accuracy_score(y_test, y_pred_adv)\n",
        "        logging.info(f\"Adversarial Accuracy: {adv_accuracy:.2f}\")\n",
        "        return adv_accuracy\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error testing adversarial robustness: {e}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        df = load_preprocessed_data('/content/unsw_nb15_preprocessed.csv')\n",
        "\n",
        "        # Optional: Downsample if dataset is too large\n",
        "        logging.info(f\"Original dataset size: {df.shape}\")\n",
        "        if df.shape[0] > 100000:  # Adjust threshold as needed\n",
        "            df = df.sample(100000, random_state=42)\n",
        "            logging.info(f\"Downsampled dataset to: {df.shape}\")\n",
        "\n",
        "        X = df.drop('binary_label', axis=1)\n",
        "        y = df['binary_label']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        logging.info(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "        rf, metrics = train_supervised_model(X_train, y_train, X_test, y_test)\n",
        "        iso_forest = train_unsupervised_model(X)\n",
        "        explain_model(rf, X_test)\n",
        "        adv_accuracy = test_adversarial_robustness(rf, X_test, y_test)\n",
        "\n",
        "        with open('metrics/metrics.txt', 'w') as f:\n",
        "            f.write(f\"Accuracy: {metrics['accuracy']:.2f}\\n\")\n",
        "            f.write(f\"F1-Score: {metrics['f1_score']:.2f}\\n\")\n",
        "            f.write(f\"False Positive Rate: {metrics['fpr']:.2f}\\n\")\n",
        "            f.write(f\"Adversarial Accuracy: {adv_accuracy:.2f}\\n\")\n",
        "\n",
        "        logging.info(\"Training complete. All metrics, plots, and models saved.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Training failed: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV9fk0d0FW_w",
        "outputId": "042e9386-58cf-45ea-9291-3f38de17c1f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.13.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Training model with GPU acceleration...\n",
            "GPU available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:27:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:27:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [09:27:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the destination folder in Google Drive\n",
        "drive_output_dir = '/content/drive/My Drive/UNSW_NB15_Outputs'\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n",
        "\n",
        "# List of files to save (including individual files and CSVs)\n",
        "files_to_save = [\n",
        "    'preprocessing.log',\n",
        "    'training.log',\n",
        "    'combined_raw.csv',\n",
        "    'missing_handled.csv',\n",
        "    'filtered_attacks.csv',\n",
        "    'features_selected.csv',\n",
        "    'encoded_categorical.csv',\n",
        "    'balanced_classes.csv',\n",
        "    'normalized_features.csv',\n",
        "    'unsw_nb15_preprocessed.csv'\n",
        "]\n",
        "\n",
        "# Explicitly add the .pkl files to ensure they are saved\n",
        "pkl_files = [\n",
        "    'models/xgb_supervised_model.pkl',\n",
        "    'models/isolation_forest_model.pkl'\n",
        "]\n",
        "\n",
        "# Combine all files to save\n",
        "files_to_save.extend(pkl_files)\n",
        "\n",
        "# List of directories to save\n",
        "directories_to_save = [\n",
        "    'models',\n",
        "    'plots',\n",
        "    'metrics'\n",
        "]\n",
        "\n",
        "# Copy individual files to Google Drive\n",
        "for file in files_to_save:\n",
        "    src_path = f'/content/{file}'\n",
        "    dst_path = f'{drive_output_dir}/{file}'\n",
        "    if os.path.exists(src_path):\n",
        "        # Ensure the destination directory exists\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "        print(f\"Copied {file} to Google Drive\")\n",
        "    else:\n",
        "        print(f\"File {file} not found, skipping\")\n",
        "\n",
        "# Copy directories to Google Drive (this will also copy the .pkl files, but we already copied them explicitly)\n",
        "for directory in directories_to_save:\n",
        "    src_path = f'/content/{directory}'\n",
        "    dst_path = f'{drive_output_dir}/{directory}'\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        print(f\"Copied {directory} directory to Google Drive\")\n",
        "    else:\n",
        "        print(f\"Directory {directory} not found, skipping\")\n",
        "\n",
        "# Save the preprocessing and training code as a .py file\n",
        "code = \"\"\"\n",
        "# Preprocessing Code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='preprocessing.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Define column names for UNSW-NB15\n",
        "COLUMNS = [\n",
        "    'srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes', 'sttl',\n",
        "    'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin',\n",
        "    'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit',\n",
        "    'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports',\n",
        "    'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src',\n",
        "    'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
        "    'ct_dst_src_ltm', 'attack_cat', 'Label'\n",
        "]\n",
        "\n",
        "def load_and_combine_data(file_paths):\n",
        "    \\\"\\\"\\\"Load and combine UNSW-NB15 CSV files.\\\"\\\"\\\"\n",
        "    try:\n",
        "        dfs = [pd.read_csv(file, names=COLUMNS, low_memory=False) for file in file_paths]\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "        logging.info(f\"Combined {len(dfs)} files. Total records: {df.shape[0]}\")\n",
        "        df.to_csv('combined_raw.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error combining files: {e}\")\n",
        "        raise\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    \\\"\\\"\\\"Handle missing values in the dataset.\\\"\\\"\\\"\n",
        "    try:\n",
        "        logging.info(f\"Missing values before: {df.isnull().sum().sum()}\")\n",
        "        df['attack_cat'] = df['attack_cat'].fillna('Normal')\n",
        "        df = df.dropna(subset=['Label'])\n",
        "        df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "        logging.info(f\"Missing values after: {df.isnull().sum().sum()}\")\n",
        "        df.to_csv('missing_handled.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error handling missing values: {e}\")\n",
        "        raise\n",
        "\n",
        "def filter_relevant_attacks(df):\n",
        "    \\\"\\\"\\\"Filter for Backdoors and Reconnaissance attacks.\\\"\\\"\\\"\n",
        "    try:\n",
        "        relevant_attacks = ['Normal', 'Backdoors', 'Reconnaissance']\n",
        "        df = df[df['attack_cat'].isin(relevant_attacks)]\n",
        "        df['binary_label'] = df['attack_cat'].apply(lambda x: 0 if x == 'Normal' else 1)\n",
        "        logging.info(f\"Filtered attacks. Records: {df.shape[0]}, Classes: {df['binary_label'].value_counts().to_dict()}\")\n",
        "        df.to_csv('filtered_attacks.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error filtering attacks: {e}\")\n",
        "        raise\n",
        "\n",
        "def select_features(df):\n",
        "    \\\"\\\"\\\"Select time and content features for supply-chain attack detection.\\\"\\\"\\\"\n",
        "    try:\n",
        "        features = [\n",
        "            'proto', 'service', 'dur', 'sbytes', 'dbytes', 'Sload', 'Dload',\n",
        "            'smeansz', 'dmeansz', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'binary_label'\n",
        "        ]\n",
        "        df = df[features]\n",
        "        logging.info(f\"Selected features: {features}\")\n",
        "        df.to_csv('features_selected.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error selecting features: {e}\")\n",
        "        raise\n",
        "\n",
        "def encode_categorical(df):\n",
        "    \\\"\\\"\\\"Encode categorical features.\\\"\\\"\\\"\n",
        "    try:\n",
        "        categorical_cols = ['proto', 'service']\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "            logging.info(f\"Encoded column: {col}\")\n",
        "        df.to_csv('encoded_categorical.csv', index=False)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error encoding categorical features: {e}\")\n",
        "        raise\n",
        "\n",
        "def balance_classes(df):\n",
        "    \\\"\\\"\\\"Balance classes using SMOTE.\\\"\\\"\\\"\n",
        "    try:\n",
        "        X = df.drop('binary_label', axis=1)\n",
        "        y = df['binary_label']\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "        df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='binary_label')], axis=1)\n",
        "        logging.info(f\"Balanced classes. New records: {df_resampled.shape[0]}, Classes: {df_resampled['binary_label'].value_counts().to_dict()}\")\n",
        "        df_resampled.to_csv('balanced_classes.csv', index=False)\n",
        "        return df_resampled\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error balancing classes: {e}\")\n",
        "        raise\n",
        "\n",
        "def normalize_features(df):\n",
        "    \\\"\\\"\\\"Normalize numerical features.\\\"\\\"\\\"\n",
        "    try:\n",
        "        X = df.drop('binary_label', axis=1)\n",
        "        scaler = StandardScaler()\n",
        "        X_normalized = scaler.fit_transform(X)\n",
        "        df_normalized = pd.concat([pd.DataFrame(X_normalized, columns=X.columns), df['binary_label'].reset_index(drop=True)], axis=1)\n",
        "        logging.info(\"Normalized features\")\n",
        "        df_normalized.to_csv('normalized_features.csv', index=False)\n",
        "        return df_normalized\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error normalizing features: {e}\")\n",
        "        raise\n",
        "\n",
        "def main_preprocessing():\n",
        "    \\\"\\\"\\\"Main preprocessing pipeline.\\\"\\\"\\\"\n",
        "    try:\n",
        "        file_paths = [\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_1.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_2.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_3.csv',\n",
        "            '/content/UNSW_DATASET/CSV Files/UNSW-NB15_4.csv'\n",
        "        ]\n",
        "\n",
        "        df = load_and_combine_data(file_paths)\n",
        "        df = handle_missing_values(df)\n",
        "        df = filter_relevant_attacks(df)\n",
        "        df = select_features(df)\n",
        "        df = encode_categorical(df)\n",
        "        df = balance_classes(df)\n",
        "        df = normalize_features(df)\n",
        "\n",
        "        df.to_csv('unsw_nb15_preprocessed.csv', index=False)\n",
        "        logging.info(\"Preprocessing complete. Final dataset saved as 'unsw_nb15_preprocessed.csv'\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Preprocessing failed: {e}\")\n",
        "        raise\n",
        "\n",
        "# Training Code\n",
        "print(\"Training model with GPU acceleration...\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import joblib\n",
        "\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('metrics', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='training.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def load_preprocessed_data(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        logging.info(f\"Loaded preprocessed data: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_supervised_model(X_train, y_train, X_test, y_test):\n",
        "    try:\n",
        "        rf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor',\n",
        "                           use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred = rf.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        fpr = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
        "\n",
        "        logging.info(f\"Supervised Model - Accuracy: {accuracy:.2f}, F1-Score: {f1:.2f}, FPR: {fpr:.2f}\")\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.savefig('plots/confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        joblib.dump(rf, 'models/xgb_supervised_model.pkl')\n",
        "\n",
        "        return rf, {'accuracy': accuracy, 'f1_score': f1, 'fpr': fpr}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training supervised model: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_unsupervised_model(X):\n",
        "    try:\n",
        "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "        anomalies = iso_forest.fit_predict(X)\n",
        "\n",
        "        df_plot = pd.DataFrame(X, columns=X.columns)\n",
        "        df_plot['Anomaly'] = anomalies\n",
        "        sns.scatterplot(data=df_plot, x='dur', y='sbytes', hue='Anomaly', palette={-1: 'red', 1: 'blue'})\n",
        "        plt.title('Anomaly Detection with Isolation Forest')\n",
        "        plt.savefig('plots/anomaly_plot.png')\n",
        "        plt.close()\n",
        "\n",
        "        joblib.dump(iso_forest, 'models/isolation_forest_model.pkl')\n",
        "\n",
        "        logging.info(\"Unsupervised model trained and anomaly plot saved\")\n",
        "        return iso_forest\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training unsupervised model: {e}\")\n",
        "        raise\n",
        "\n",
        "def explain_model(rf, X_test):\n",
        "    try:\n",
        "        explainer = shap.Explainer(rf)\n",
        "        shap_values = explainer(X_test)\n",
        "        shap.summary_plot(shap_values, X_test, show=False)\n",
        "        plt.savefig('plots/shap_summary.png')\n",
        "        plt.close()\n",
        "        logging.info(\"SHAP summary plot saved\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating SHAP explanations: {e}\")\n",
        "        raise\n",
        "\n",
        "def test_adversarial_robustness(rf, X_test, y_test):\n",
        "    try:\n",
        "        X_adv = X_test + np.random.normal(0, 0.1, X_test.shape)\n",
        "        y_pred_adv = rf.predict(X_adv)\n",
        "        adv_accuracy = accuracy_score(y_test, y_pred_adv)\n",
        "        logging.info(f\"Adversarial Accuracy: {adv_accuracy:.2f}\")\n",
        "        return adv_accuracy\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error testing adversarial robustness: {e}\")\n",
        "        raise\n",
        "\n",
        "def main_training():\n",
        "    try:\n",
        "        df = load_preprocessed_data('/content/unsw_nb15_preprocessed.csv')\n",
        "\n",
        "        logging.info(f\"Original dataset size: {df.shape}\")\n",
        "        if df.shape[0] > 100000:\n",
        "            df = df.sample(100000, random_state=42)\n",
        "            logging.info(f\"Downsampled dataset to: {df.shape}\")\n",
        "\n",
        "        X = df.drop('binary_label', axis=1)\n",
        "        y = df['binary_label']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        logging.info(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "        rf, metrics = train_supervised_model(X_train, y_train, X_test, y_test)\n",
        "        iso_forest = train_unsupervised_model(X)\n",
        "        explain_model(rf, X_test)\n",
        "        adv_accuracy = test_adversarial_robustness(rf, X_test, y_test)\n",
        "\n",
        "        with open('metrics/metrics.txt', 'w') as f:\n",
        "            f.write(f\"Accuracy: {metrics['accuracy']:.2f}\\\\n\")\n",
        "            f.write(f\"F1-Score: {metrics['f1_score']:.2f}\\\\n\")\n",
        "            f.write(f\"False Positive Rate: {metrics['fpr']:.2f}\\\\n\")\n",
        "            f.write(f\"Adversarial Accuracy: {adv_accuracy:.2f}\\\\n\")\n",
        "\n",
        "        logging.info(\"Training complete. All metrics, plots, and models saved.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Training failed: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_preprocessing()\n",
        "    main_training()\n",
        "\"\"\"\n",
        "\n",
        "# Save the code to a .py file in Colab\n",
        "code_file_path = '/content/unsw_nb15_code.py'\n",
        "with open(code_file_path, 'w') as f:\n",
        "    f.write(code)\n",
        "\n",
        "# Copy the code file to Google Drive\n",
        "dst_code_path = f'{drive_output_dir}/unsw_nb15_code.py'\n",
        "shutil.copy(code_file_path, dst_code_path)\n",
        "print(f\"Copied code file to Google Drive at {dst_code_path}\")\n",
        "\n",
        "# Verify the .pkl files were copied\n",
        "for pkl_file in pkl_files:\n",
        "    drive_pkl_path = f'{drive_output_dir}/{pkl_file}'\n",
        "    if os.path.exists(drive_pkl_path):\n",
        "        print(f\"Verified: {pkl_file} exists in Google Drive at {drive_pkl_path}\")\n",
        "    else:\n",
        "        print(f\"Error: {pkl_file} not found in Google Drive\")\n",
        "\n",
        "print(\"All outputs, including models (.pkl files) and code, have been saved to Google Drive!\")"
      ],
      "metadata": {
        "id": "33RVg8tmGUrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a16da64-61ab-44a4-caaa-d912628e6474"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File preprocessing.log not found, skipping\n",
            "File training.log not found, skipping\n",
            "Copied combined_raw.csv to Google Drive\n",
            "Copied missing_handled.csv to Google Drive\n",
            "Copied filtered_attacks.csv to Google Drive\n",
            "Copied features_selected.csv to Google Drive\n",
            "Copied encoded_categorical.csv to Google Drive\n",
            "Copied balanced_classes.csv to Google Drive\n",
            "Copied normalized_features.csv to Google Drive\n",
            "Copied unsw_nb15_preprocessed.csv to Google Drive\n",
            "Copied models/xgb_supervised_model.pkl to Google Drive\n",
            "Copied models/isolation_forest_model.pkl to Google Drive\n",
            "Copied models directory to Google Drive\n",
            "Copied plots directory to Google Drive\n",
            "Copied metrics directory to Google Drive\n",
            "Copied code file to Google Drive at /content/drive/My Drive/UNSW_NB15_Outputs/unsw_nb15_code.py\n",
            "Verified: models/xgb_supervised_model.pkl exists in Google Drive at /content/drive/My Drive/UNSW_NB15_Outputs/models/xgb_supervised_model.pkl\n",
            "Verified: models/isolation_forest_model.pkl exists in Google Drive at /content/drive/My Drive/UNSW_NB15_Outputs/models/isolation_forest_model.pkl\n",
            "All outputs, including models (.pkl files) and code, have been saved to Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwvWUUFwJzSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}